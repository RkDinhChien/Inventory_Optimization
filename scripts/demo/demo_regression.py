"""
Demo: CÃ´ng Thá»©c Dá»± ÄoÃ¡n - CÃ³ DÃ¹ng Regression KhÃ´ng?
Giáº£i thÃ­ch chi tiáº¿t toÃ¡n há»c Ä‘áº±ng sau ML forecasting
"""

print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘           CÃ”NG THá»¨C Dá»° ÄOÃN - REGRESSION & MACHINE LEARNING              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

print("""
â“ CÃ‚U Há»I: "CÃ´ng thá»©c dá»± Ä‘oÃ¡n lÃ  gÃ¬? CÃ³ dÃ¹ng regression khÃ´ng?"
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

âœ… CÃ‚U TRáº¢ Lá»œI: CÃ“! Táº¥t cáº£ Ä‘á»u dÃ¹ng regression (dá»± Ä‘oÃ¡n sá»‘ liÃªn tá»¥c)

NhÆ°ng cÃ³ nhiá»u LOáº I regression khÃ¡c nhau:
  1. Linear Regression (Ä‘Æ¡n giáº£n)
  2. Tree-based Regression (Random Forest, XGBoost)
  3. Time Series Regression (SARIMA, Prophet)
""")

print("""
ğŸ“Š 1. STATISTICAL METHOD (Simple Linear Approach)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

CÃ´ng thá»©c:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    y = Î¼ Ã— s Ã— w
    
    Trong Ä‘Ã³:
    â€¢ y = predicted quantity (sá»‘ lÆ°á»£ng dá»± Ä‘oÃ¡n)
    â€¢ Î¼ = historical mean (trung bÃ¬nh lá»‹ch sá»­)
    â€¢ s = seasonal_factor (há»‡ sá»‘ mÃ¹a)
    â€¢ w = weekend_factor (há»‡ sá»‘ cuá»‘i tuáº§n)

VÃ­ dá»¥ cá»¥ thá»ƒ:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    Data lá»‹ch sá»­: Pasta Marinara bÃ¡n trung bÃ¬nh 50 pháº§n/ngÃ y
    
    Dá»± Ä‘oÃ¡n cho Thá»© 7, thÃ¡ng 12 (mÃ¹a Ä‘Ã´ng):
    
    Î¼ = 50 (trung bÃ¬nh)
    s = 1.1 (mÃ¹a Ä‘Ã´ng +10%)
    w = 1.2 (cuá»‘i tuáº§n +20%)
    
    y = 50 Ã— 1.1 Ã— 1.2 = 66 pháº§n
    
    âœ“ ÄÃ¢y lÃ  Linear Regression Ä‘Æ¡n giáº£n (chá»‰ nhÃ¢n há»‡ sá»‘)
    âœ— KhÃ´ng há»c Ä‘Æ°á»£c patterns phá»©c táº¡p
    âœ— Giáº£ Ä‘á»‹nh relationship lÃ  linear (tuyáº¿n tÃ­nh)
""")

print("""
ğŸ¯ 2. XGBOOST (Gradient Boosted Regression Trees)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

CÃ´ng thá»©c tá»•ng quÃ¡t:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    Å· = f(X) = Î£(k=1 to K) f_k(X)
    
    Trong Ä‘Ã³:
    â€¢ Å· = predicted value
    â€¢ X = feature vector [xâ‚, xâ‚‚, ..., xâ‚â‚‡]
    â€¢ f_k = tree thá»© k
    â€¢ K = sá»‘ trees (thÆ°á»ng 100-1000 trees)

Chi tiáº¿t hÆ¡n:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    Å· = fâ‚(X) + fâ‚‚(X) + fâ‚ƒ(X) + ... + f_K(X)
    
    Má»—i tree f_k há»c tá»« "residual" (sai sá»‘) cá»§a tree trÆ°á»›c:
    
    Tree 1: fâ‚(X) â†’ dá»± Ä‘oÃ¡n ban Ä‘áº§u
    Residual 1: râ‚ = y_actual - fâ‚(X)
    
    Tree 2: fâ‚‚(X) â†’ há»c tá»« râ‚
    Residual 2: râ‚‚ = râ‚ - fâ‚‚(X)
    
    Tree 3: fâ‚ƒ(X) â†’ há»c tá»« râ‚‚
    ...
    
    Káº¿t quáº£ cuá»‘i: Å· = Î£(k=1 to K) f_k(X)

Feature vector X (17 features):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    X = [
        day_of_week,      # 0-6 (Mon-Sun)
        day_of_month,     # 1-31
        month,            # 1-12
        quarter,          # 1-4
        week_of_year,     # 1-52
        day_of_year,      # 1-365
        is_weekend,       # 0 or 1
        day_sin,          # sin(2Ï€ Ã— day/365)
        day_cos,          # cos(2Ï€ Ã— day/365)
        month_sin,        # sin(2Ï€ Ã— month/12)
        month_cos,        # cos(2Ï€ Ã— month/12)
        is_month_start,   # 0 or 1
        is_month_end,     # 0 or 1
        is_quarter_start, # 0 or 1
        is_quarter_end,   # 0 or 1
        is_year_start,    # 0 or 1
        is_year_end       # 0 or 1
    ]

VÃ­ dá»¥ cá»¥ thá»ƒ:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    Dá»± Ä‘oÃ¡n cho ngÃ y 15/12/2024 (Chá»§ nháº­t):
    
    X = [6, 15, 12, 4, 50, 350, 1, 0.95, 0.31, 0.0, 1.0, 0, 0, 0, 0, 0, 0]
    
    Tree 1: Kiá»ƒm tra "is_weekend == 1" â†’ +12 pháº§n
    Tree 2: Kiá»ƒm tra "month == 12" â†’ +5 pháº§n
    Tree 3: Kiá»ƒm tra "day_of_month == 15" â†’ +3 pháº§n
    ...
    Tree 100: Tá»•ng há»£p táº¥t cáº£ â†’ +2 pháº§n
    
    Å· = 50 (base) + 12 + 5 + 3 + ... + 2 = 72 pháº§n
    
    âœ“ Non-linear regression (khÃ´ng tuyáº¿n tÃ­nh)
    âœ“ Há»c Ä‘Æ°á»£c interactions phá»©c táº¡p
    âœ“ Tá»± Ä‘á»™ng feature importance
""")

print("""
ğŸŒ² 3. CHI TIáº¾T Vá»€ DECISION TREE REGRESSION
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Má»—i tree lÃ  má»™t hÃ m phÃ¢n Ä‘oáº¡n (piecewise function):

    f(X) = {
        wâ‚, if X âˆˆ Regionâ‚
        wâ‚‚, if X âˆˆ Regionâ‚‚
        wâ‚ƒ, if X âˆˆ Regionâ‚ƒ
        ...
    }

VÃ­ dá»¥ Decision Tree:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

                    [Root: is_weekend?]
                    /                 \\
                Yes (1)              No (0)
                /                      \\
        [month >= 11?]          [day_of_week >= 5?]
        /           \\              /              \\
      Yes          No            Yes              No
      /            \\            /                \\
    Predict      Predict     Predict          Predict
     +20          +15         +10              +5

CÃ´ng thá»©c toÃ¡n há»c:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    f(X) = Î£(j=1 to J) w_j Ã— I(X âˆˆ R_j)
    
    Trong Ä‘Ã³:
    â€¢ J = sá»‘ leaf nodes (terminal nodes)
    â€¢ w_j = weight cá»§a leaf j
    â€¢ R_j = region j (Ä‘iá»u kiá»‡n Ä‘á»ƒ Ä‘áº¿n leaf j)
    â€¢ I(.) = indicator function (1 náº¿u true, 0 náº¿u false)

VÃ­ dá»¥ vá»›i tree trÃªn:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    f(X) = 20 Ã— I(is_weekend=1 AND monthâ‰¥11)
         + 15 Ã— I(is_weekend=1 AND month<11)
         + 10 Ã— I(is_weekend=0 AND day_of_weekâ‰¥5)
         + 5  Ã— I(is_weekend=0 AND day_of_week<5)
    
    Náº¿u X = [is_weekend=1, month=12, day_of_week=6]:
    f(X) = 20 Ã— 1 + 15 Ã— 0 + 10 Ã— 0 + 5 Ã— 0 = 20
""")

print("""
ğŸ“ 4. GRADIENT BOOSTING - TOÃN Há»ŒC
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Objective Function (HÃ m má»¥c tiÃªu):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    L = Î£(i=1 to n) l(y_i, Å·_i) + Î£(k=1 to K) Î©(f_k)
    
    Trong Ä‘Ã³:
    â€¢ L = Total loss (tá»•ng loss)
    â€¢ l(y_i, Å·_i) = loss function (MSE, MAE, etc.)
    â€¢ Î©(f_k) = regularization term (penalty cho complexity)
    â€¢ n = sá»‘ samples
    â€¢ K = sá»‘ trees

Loss Function (MSE cho regression):
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    l(y, Å·) = (y - Å·)Â²
    
    VD: y = 50 (thá»±c táº¿), Å· = 48 (dá»± Ä‘oÃ¡n)
    l = (50 - 48)Â² = 4

Regularization Term:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    Î©(f) = Î³T + (Î»/2) Î£(j=1 to T) w_jÂ²
    
    Trong Ä‘Ã³:
    â€¢ T = sá»‘ leaves
    â€¢ w_j = weight cá»§a leaf j
    â€¢ Î³ = penalty cho sá»‘ leaves
    â€¢ Î» = L2 regularization

Gradient Descent Update:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    Å·â½áµ—â¾ = Å·â½áµ—â»Â¹â¾ - Î· Ã— âˆ‚L/âˆ‚Å·
    
    Trong Ä‘Ã³:
    â€¢ Å·â½áµ—â¾ = prediction á»Ÿ iteration t
    â€¢ Î· = learning rate (thÆ°á»ng 0.1-0.3)
    â€¢ âˆ‚L/âˆ‚Å· = gradient (Ä‘áº¡o hÃ m)

Vá»›i MSE:
â”€â”€â”€â”€â”€â”€â”€â”€

    âˆ‚L/âˆ‚Å· = -2(y - Å·) = -2 Ã— residual
    
    VD: y = 50, Å· = 48
    gradient = -2 Ã— (50 - 48) = -4
    
    Å·_new = 48 - 0.1 Ã— (-4) = 48.4
""")

print("""
ğŸ”¢ 5. VÃ Dá»¤ TÃNH TOÃN THá»°C Táº¾
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Scenario: Dá»± Ä‘oÃ¡n sá»‘ lÆ°á»£ng Pasta cho ngÃ y 15/12/2024 (Chá»§ nháº­t)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Input Features:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    X = {
        day_of_week: 6 (Sunday),
        day_of_month: 15,
        month: 12,
        quarter: 4,
        week_of_year: 50,
        day_of_year: 350,
        is_weekend: 1,
        day_sin: sin(2Ï€ Ã— 350/365) = 0.95,
        day_cos: cos(2Ï€ Ã— 350/365) = 0.31,
        month_sin: sin(2Ï€ Ã— 12/12) = 0.0,
        month_cos: cos(2Ï€ Ã— 12/12) = 1.0,
        is_month_start: 0,
        is_month_end: 0,
        is_quarter_start: 0,
        is_quarter_end: 0,
        is_year_start: 0,
        is_year_end: 0
    }

XGBoost Prediction Process:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    Base prediction: Å·â‚€ = mean(training_data) = 50 pháº§n
    
    Tree 1 (depth=3, 8 leaves):
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â€¢ Kiá»ƒm tra: is_weekend == 1 â†’ Go right
    â€¢ Kiá»ƒm tra: month >= 11 â†’ Go right  
    â€¢ Kiá»ƒm tra: day_of_month >= 10 â†’ Go right
    â€¢ Reach leaf: wâ‚ = +12
    
    Å·â‚ = 50 + 0.1 Ã— 12 = 51.2  (learning_rate = 0.1)
    
    Tree 2:
    â”€â”€â”€â”€â”€â”€â”€
    â€¢ Learn tá»« residual: râ‚ = y_true - Å·â‚
    â€¢ Pattern: "Sunday in December" â†’ wâ‚‚ = +8
    
    Å·â‚‚ = 51.2 + 0.1 Ã— 8 = 52.0
    
    Tree 3:
    â”€â”€â”€â”€â”€â”€â”€
    â€¢ Pattern: "Mid-month weekend" â†’ wâ‚ƒ = +5
    
    Å·â‚ƒ = 52.0 + 0.1 Ã— 5 = 52.5
    
    ...
    
    Tree 100:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€
    â€¢ Fine-tuning adjustment â†’ wâ‚â‚€â‚€ = +2
    
    Å·â‚â‚€â‚€ = 71.8 + 0.1 Ã— 2 = 72.0
    
    Final Prediction: Å· = 72 pháº§n

Mathematical Formula:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    Å· = Å·â‚€ + Î· Ã— Î£(k=1 to 100) f_k(X)
      = 50 + 0.1 Ã— (12 + 8 + 5 + ... + 2)
      = 50 + 0.1 Ã— 220
      = 50 + 22
      = 72 pháº§n
""")

print("""
ğŸ“Š 6. SO SÃNH CÃC LOáº I REGRESSION
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Method             â”‚ Formula Type         â”‚ Complexity         â”‚ Accuracy â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Statistical        â”‚ Linear               â”‚ O(1) - instant     â”‚ 75-80%   â”‚
â”‚ (Î¼ Ã— s Ã— w)        â”‚ y = ax + b           â”‚                    â”‚          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Linear Regression  â”‚ Linear               â”‚ O(n)               â”‚ 70-75%   â”‚
â”‚ y = wâ‚€ + Î£w_ix_i   â”‚ Multiple vars        â”‚                    â”‚          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Random Forest      â”‚ Non-linear           â”‚ O(n log n)         â”‚ 85-92%   â”‚
â”‚ Ensemble trees     â”‚ Piecewise constant   â”‚                    â”‚          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ XGBoost            â”‚ Non-linear           â”‚ O(n log n)         â”‚ 90-95%   â”‚
â”‚ Gradient boosting  â”‚ Additive model       â”‚                    â”‚ ğŸ†       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ SARIMA             â”‚ Time series          â”‚ O(nÂ²)              â”‚ 85-90%   â”‚
â”‚ AR + MA + Season   â”‚ Auto-regressive      â”‚                    â”‚          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Feature Engineering:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    Statistical:     3 features (mean, season, weekend)
    Linear Reg:      5-7 features (manual selection)
    Random Forest:   10-15 features (automatic selection)
    XGBoost:        17 features (optimized encoding) ğŸ†
    SARIMA:         Time lags only (no external features)
""")

print("""
ğŸ“ 7. Táº I SAO XGBOOST Tá»T HÆ N?
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

1ï¸âƒ£  Non-linear Relationships:
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   
   Statistical: y = 50 Ã— 1.1 Ã— 1.2 = 66 (linear)
   â†’ Giáº£ Ä‘á»‹nh mÃ¹a Ä‘Ã´ng vÃ  cuá»‘i tuáº§n NHÃ‚N vá»›i nhau
   
   XGBoost: Há»c Ä‘Æ°á»£c "Sunday in December" â‰  "Saturday in December"
   â†’ Há»c interactions phá»©c táº¡p giá»¯a features

2ï¸âƒ£  Gradient Boosting (Há»c tá»« sai sá»‘):
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   
   Tree 1: Dá»± Ä‘oÃ¡n 50 â†’ Sai 10
   Tree 2: Há»c tá»« sai sá»‘ 10 â†’ Fix Ä‘Æ°á»£c 8
   Tree 3: Há»c tá»« sai sá»‘ 2 â†’ Fix Ä‘Æ°á»£c 1.5
   ...
   â†’ Má»—i tree cáº£i thiá»‡n predictions

3ï¸âƒ£  Automatic Feature Interaction:
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   
   Statistical: Manual há»‡ sá»‘ (mÃ¹a Ã— weekend)
   XGBoost: Tá»± Ä‘á»™ng tÃ¬m "is_weekend=1 AND month=12 AND dayâ‰¥15"
   â†’ PhÃ¡t hiá»‡n patterns mÃ  con ngÆ°á»i khÃ´ng nghÄ© Ä‘áº¿n

4ï¸âƒ£  Regularization:
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   
   Î©(f) = Î³T + (Î»/2)Î£wÂ²
   â†’ TrÃ¡nh overfitting
   â†’ Generalize tá»‘t cho data má»›i

5ï¸âƒ£  Handling Missing Data:
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   
   XGBoost tá»± Ä‘á»™ng xá»­ lÃ½ missing values
   â†’ KhÃ´ng cáº§n imputation
   â†’ Há»c Ä‘Æ°á»£c "missing-ness" cÅ©ng lÃ  feature
""")

print("""
âœ… Káº¾T LUáº¬N - TRáº¢ Lá»œI CÃ‚U Há»I
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Q: "CÃ´ng thá»©c dá»± Ä‘oÃ¡n lÃ  gÃ¬?"
A: 
   Statistical: y = Î¼ Ã— s Ã— w (linear multiplication)
   XGBoost:     Å· = Î£(k=1 to K) f_k(X) (additive ensemble)

Q: "CÃ³ dÃ¹ng regression khÃ´ng?"
A: 
   CÃ“! Táº¥t cáº£ Ä‘á»u lÃ  regression (dá»± Ä‘oÃ¡n sá»‘ liÃªn tá»¥c):
   â€¢ Statistical â†’ Simple Linear Regression
   â€¢ XGBoost â†’ Gradient Boosted Regression Trees
   â€¢ SARIMA â†’ Autoregressive Time Series Regression
   â€¢ Random Forest â†’ Ensemble Regression Trees

Q: "Regression nÃ o tá»‘t nháº¥t?"
A: 
   XGBoost Gradient Boosted Regression ğŸ†
   â€¢ Non-linear
   â€¢ Learns from errors
   â€¢ Automatic feature interactions
   â€¢ 90-95% accuracy

ğŸ“ CÃ´ng thá»©c XGBoost Ä‘áº§y Ä‘á»§:
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    Å· = Î£(k=1 to K) f_k(X)
    
    Vá»›i:
    â€¢ f_k(X) = Î£(j=1 to T_k) w_j,k Ã— I(X âˆˆ R_j,k)
    â€¢ Minimize: L = Î£ l(y_i, Å·_i) + Î£ Î©(f_k)
    â€¢ Update: Å·â½áµ—â¾ = Å·â½áµ—â»Â¹â¾ - Î· Ã— âˆ‡L
    
    â†’ Phá»©c táº¡p nhÆ°ng chÃ­nh xÃ¡c nháº¥t!
""")

print("""
ğŸš€ DEMO THá»°C Táº¾
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

Muá»‘n tháº¥y XGBoost regression hoáº¡t Ä‘á»™ng?

1ï¸âƒ£  Cháº¡y comparison demo:
   $ python demo_comparison.py
   
   â†’ Tháº¥y rÃµ cÃ´ng thá»©c nÃ o chÃ­nh xÃ¡c hÆ¡n
   â†’ Statistical: 830 servings (simple formula)
   â†’ XGBoost: 747 servings (complex regression)

2ï¸âƒ£  Test trÃªn web app:
   $ streamlit run app.py
   
   â†’ Toggle ML ON
   â†’ Chá»n XGBoost
   â†’ Xem prediction process

3ï¸âƒ£  Code implementation:
   â†’ src/ml_forecaster.py (385 lines)
   â†’ Xem chi tiáº¿t 17 features
   â†’ Xem XGBoost training code

ğŸ’¡ Key Takeaway:
   Regression khÃ´ng chá»‰ lÃ  y = ax + b Ä‘Æ¡n giáº£n!
   XGBoost = Advanced non-linear regression vá»›i 100+ trees! ğŸŒ²
""")
